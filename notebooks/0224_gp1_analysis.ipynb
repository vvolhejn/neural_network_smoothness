{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices([gpus[1]], 'GPU')\n",
    "\n",
    "sys.path.append(\"/nfs/scistore12/chlgrp/vvolhejn/smooth\")\n",
    "\n",
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0224_gp1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"measures.feather\")\n",
    "smooth.analysis.remove_constant_columns(df)\n",
    "df = smooth.analysis.expand_dataset_columns(df)\n",
    "df[\"log_dir\"] = df[\"log_dir\"].str.split(\"/\").str.get(-1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"actual_epochs\", \"train_loss\", \"test_loss\"]:\n",
    "    if df[col].dtype == \"object\":\n",
    "        continue\n",
    "    plt.hist(df[col], bins=20)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[(df[\"seed\"] == 1) & (df[\"lengthscale\"] == 0.1) & (df[\"train_loss\"] < 10)]\n",
    "plt.scatter(df1[\"samples_train\"], df1[\"seg_total_variation\"], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "These models are larger and so I was hoping they could successfully fit datasets with `lengthscale == 0.1`.\n",
    "This is not the case, however, as we can see from the plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[(df[\"seed\"] == 4) & (df[\"lengthscale\"] == 0.3) & (df[\"train_loss\"] < 0.01)]\n",
    "# df1 = df1[(df1[\"\"])]\n",
    "\n",
    "# plt.scatter(df1[\"samples_train\"], df1[\"seg_total_variation\"], alpha=0.3)\n",
    "df1 = df1[df1[\"samples_train\"] == 100]\n",
    "print(len(df1))\n",
    "\n",
    "dataset = smooth.datasets.GaussianProcessDataset.from_name(df1.iloc[0][\"dataset\"])\n",
    "x = dataset.x_test\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.plot(x, dataset.y_test, color=\"C0\")\n",
    "\n",
    "for i, row in list(df1.iterrows()):\n",
    "    log_dir = row[\"log_dir\"]\n",
    "    model = tf.keras.models.load_model(os.path.join(log_dir, \"model.h5\"))\n",
    "    y = model.predict(x)\n",
    "    color = {\n",
    "        10: \"C1\",\n",
    "        30: \"C2\",\n",
    "        100: \"C3\",\n",
    "    }[row[\"samples_train\"]]\n",
    "    ax.plot(x, y, alpha=0.1, color=color)\n",
    "\n",
    "\n",
    "#     smooth.analysis.plot_shallow(model, dataset, title=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which hyperparameters work the best across datasets?\n",
    "\n",
    "For each dataset, find which hyperparameters were the most successful in terms of training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = df.sort_values(\"train_loss\").groupby(\"dataset\").first()[[\"init_scale\", \"learning_rate\"]]\n",
    "# hps[\"hp\"] = hp\n",
    "groups0 = hps.groupby([\"init_scale\", \"learning_rate\"], as_index=False)\n",
    "groups = {}\n",
    "for hp, g_df in groups0:\n",
    "    groups[hp] = len(g_df)\n",
    "\n",
    "print(\"Which hyperparameters are the best?\")\n",
    "groups\n",
    "# .first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
