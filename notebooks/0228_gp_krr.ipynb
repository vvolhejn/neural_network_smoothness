{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel ridge regression on GP datasets 1\n",
    "\n",
    "The problem here is that the datasets are \"too hard\" -- for lengthscale 1.0, if we use the original GP to predict the test set given the training set, the variance is 1., so we can't expect any model to do better than that. In a follow-up analysis, we use models with `lengthscale == c * dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices([gpus[1]], 'GPU')\n",
    "\n",
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0228_gp_krr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_feather(\"measures.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = pd.read_feather(\"measures.feather\")\n",
    "ms = smooth.analysis.expand_dataset_columns(ms)\n",
    "smooth.analysis.remove_constant_columns(ms, verbose=True)\n",
    "ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim = 0.1\n",
    "\n",
    "for col in ms.columns:\n",
    "    if ms[col].dtype == \"object\":\n",
    "        continue\n",
    "    \n",
    "    data = ms.loc[(ms[col] >= ms[col].quantile(trim/2)) & (ms[col] <= ms[col].quantile(1-trim/2)), col]\n",
    "    \n",
    "    plt.hist(data, bins=20)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_path_length_f(dataset_name):\n",
    "    dataset = smooth.datasets.from_name(dataset_name)\n",
    "    n = len(dataset.x_test)\n",
    "    y = sorted(dataset.y_test.reshape((-1,)))\n",
    "#     cs = np.cumsum(y)[::-1]\n",
    "#     res = 0\n",
    "#     for i in range(n - 1):\n",
    "#         res += cs[i] - y[i] * (n - i)\n",
    "\n",
    "#     return res / (n ** 2)\n",
    "    res = 0\n",
    "    for a in y:\n",
    "        for b in y:\n",
    "            res += np.abs(a - b)\n",
    "    return res / (n ** 2)\n",
    "\n",
    "\n",
    "datasets = ms[\"dataset\"].str.split(\"-\").str.slice(0, -1).str.join(\"-\").unique()\n",
    "datasets\n",
    "\n",
    "optimal_lengths = {}\n",
    "for dataset in tqdm.notebook.tqdm(datasets):\n",
    "    optimal_lengths[dataset] = get_optimal_path_length_f(\"{}-77\".format(dataset))\n",
    "\n",
    "optimal_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms1 = ms\n",
    "seed = 2\n",
    "measure_cols = [\"train_loss\", \"test_loss\", \"path_length_f\"]\n",
    "# measure_cols = [\"path_length_f\"]\n",
    "\n",
    "for dim in sorted(ms[\"dim\"].unique()):\n",
    "    for measure in measure_cols:\n",
    "        ms1 = ms[(ms[\"seed\"] == seed) & (ms[\"dim\"] == dim) & (ms[\"alpha\"] == 0.0001)]\n",
    "        ax = plt.subplot()\n",
    "    #     ms1.loc[:,\"hidden_size_s\"] = ms1[\"hidden_size\"].astype(str) + \" units\"\n",
    "        sns.lineplot(\n",
    "            data=ms1,\n",
    "            x=\"samples_train\",\n",
    "            y=measure,\n",
    "            hue=\"degree\",\n",
    "    #         col=\"batch_size\",\n",
    "    #         kind=\"line\",\n",
    "            palette=smooth.analysis.make_palette(ms1[\"degree\"].unique()),\n",
    "    #                 ax=ax\n",
    "        )\n",
    "        if measure == \"path_length_f\":\n",
    "            ol = optimal_lengths[\"gp-{}-{}-1.0\".format(dim, seed)]\n",
    "            plt.plot([ms1[\"samples_train\"].min(), ms1[\"samples_train\"].max()], [ol, ol])\n",
    "\n",
    "        ax.set_xscale(\"log\")\n",
    "        if \"loss\" in measure:\n",
    "            ax.set_yscale(\"log\")\n",
    "        plt.title(\"dim={}\".format(dim))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for seed in tqdm.notebook.tqdm(range(1, 6)):\n",
    "    datasets[seed] = smooth.datasets.from_name(\"gp-100-{}-1.0-1000\".format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRRModel:\n",
    "    \n",
    "    def __init__(self, krr):\n",
    "        self.krr = krr\n",
    "    \n",
    "    def predict(self, x, batch_size=None):\n",
    "        # batch_size is a fake argument which is ignored\n",
    "        return self.krr.predict(x)\n",
    "\n",
    "\n",
    "def measure_krr(krr, dataset):\n",
    "    def mse(y1, y2):\n",
    "        return np.mean(tf.losses.mean_squared_error(y1, y2))\n",
    "\n",
    "    train_loss = mse(krr.predict(dataset.x_train), dataset.y_train)\n",
    "    test_loss = mse(krr.predict(dataset.x_test), dataset.y_test)\n",
    "    path_length_f = smooth.measures.path_length(KRRModel(krr), dataset.x_test)\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"path_length_f\": path_length_f,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sklearn.kernel_ridge\n",
    "import warnings\n",
    "return\n",
    "ms_krr_l = []\n",
    "\n",
    "samples_l = np.logspace(np.log10(10), np.log10(1000), 50).round().astype(int)\n",
    "seed = 1\n",
    "\n",
    "for dim in tqdm.notebook.tqdm([4, 8, 16], desc=\"dim\"):\n",
    "    for alpha in tqdm.notebook.tqdm([0.01, 0.0001, 1e-15], leave=False, desc=\"alpha\"):\n",
    "        dataset0 = smooth.datasets.from_name(\"gp-{}-{}-{}-1000\".format(dim, seed, dim))\n",
    "        for degree in tqdm.notebook.tqdm([1, 2, 3, 4, 5], leave=False, desc=\"degree\"):\n",
    "            for samples in tqdm.notebook.tqdm(samples_l, leave=False, desc=\"samples\"):            \n",
    "                krr = sklearn.kernel_ridge.KernelRidge(\n",
    "                    alpha=alpha,\n",
    "                    kernel=\"poly\",\n",
    "                    degree=degree,\n",
    "            #     degree=len(dataset.x_train) + 10,\n",
    "                    coef0=1,\n",
    "                )\n",
    "                dataset = dataset0.subset(samples, keep_test_set=True)\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    krr.fit(dataset.x_train, dataset.y_train)\n",
    "\n",
    "                m = smooth.train_kernel_models.measure_krr(krr, dataset)\n",
    "                m.update(\n",
    "                    dim=dim,\n",
    "                    seed=seed,\n",
    "                    alpha=alpha,\n",
    "                    degree=degree,\n",
    "                    samples_train=samples,\n",
    "                )\n",
    "                ms_krr_l.append(m)\n",
    "            #     y_pred = krr.predict(dataset.x_test)\n",
    "            #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros_like(dataset.y_test) + np.mean(dataset.y_test)\n",
    "sklearn.metrics.mean_squared_error(dataset.y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary experiments with updated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "ms_krr = pd.DataFrame(ms_krr_l)\n",
    "# ms_krr.loc[range(0, 1500), \"dim\"] = np.array([4,8,16])[np.array(range(1500)) // 500]\n",
    "# ms_krr[\"dim\"] = np.array([4,8,16])[ms_krr.index // 500]\n",
    "# ms_krr = ms_krr.loc[ms_krr[\"alpha\"] < 1e-9]\n",
    "# ms_krr[\"samples_train\"] = np.sort(ms[\"samples_train\"].unique())[ms_krr.index % 10]\n",
    "# ms_krr = ms_krr[ms_krr[\"degree\"] == 3]\n",
    "\n",
    "for measure in [\"train_loss\", \"test_loss\", \"path_length_f_test\", \"path_length_f_train\"]:\n",
    "#     ax = plt.subplot()\n",
    "    grid = sns.relplot(\n",
    "        data=ms_krr,\n",
    "        x=\"samples_train\",\n",
    "        y=measure,\n",
    "        hue=\"degree\",\n",
    "        style=\"alpha\",\n",
    "        col=\"dim\",\n",
    "        kind=\"line\",\n",
    "        palette=smooth.analysis.make_palette(ms_krr[\"degree\"].unique()),\n",
    "    #         sns.cubehelix_palette(8),\n",
    "    #                 ax=ax\n",
    "    )\n",
    "    ax = grid.axes[0][0]\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    if measure == \"test_loss\":\n",
    "        baseline = sklearn.metrics.mean_squared_error(dataset.y_test, y_pred)\n",
    "        plt.plot([ms[\"samples_train\"].min(), ms[\"samples_train\"].max()], [baseline, baseline])\n",
    "\n",
    "#     ax.set_xscale(\"log\")\n",
    "#     if measure in [\"train_loss\", \"test_loss\", \"path_length_f\"]:\n",
    "#         ax.set_yscale(\"log\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The datasets are too hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = {}\n",
    "for dim in tqdm.notebook.tqdm([2**i for i in range(1, 10)]):\n",
    "    lengthscale = 1.0\n",
    "    d = smooth.datasets.from_name(\"gp-{}-1-{}-1000\".format(dim, lengthscale))\n",
    "    d.gp_model.set_XY(d.x_train, d.y_train)\n",
    "    vs[dim] = np.mean(d.gp_model.predict(d.x_test)[1])\n",
    "    print(vs[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
