{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measures on MNIST when we gradually increase training samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices([gpus[1]], 'GPU')\n",
    "\n",
    "sys.path.append(\"/nfs/scistore12/chlgrp/vvolhejn/smooth\")\n",
    "\n",
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0226_mnist/\")\n",
    "# os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0227-104002/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_raw = pd.read_feather(\"measures.feather\")\n",
    "\n",
    "ms_raw = smooth.analysis.expand_dataset_columns(ms_raw)\n",
    "ms = ms_raw\n",
    "\n",
    "# divergent_model_mask = (ms[\"loss\"] == np.inf) | (~(ms[\"train_loss\"] < 0.1))\n",
    "# print(\"Divergent models:\", len(ms[divergent_model_mask]))\n",
    "# ms = ms.loc[~divergent_model_mask]\n",
    "\n",
    "print(\"Remaining:\", len(ms))\n",
    "smooth.analysis.remove_constant_columns(ms, verbose=True)\n",
    "# ms = smooth.analysis.expand_dataset_columns(ms)\n",
    "ms.loc[:,\"log_dir\"] = ms[\"log_dir\"].str.split(\"/\").str.get(-1)\n",
    "\n",
    "# for d in sorted(ms_raw[\"dim\"].unique()):\n",
    "#     n_before = len(ms_raw[ms_raw[\"dim\"] == d])\n",
    "#     n_after = len(ms[ms[\"dim\"] == d])\n",
    "#     print(\"For dim {}:\\t{}/{}\\t({:.0f}%) remain\".format(d, n_after, n_before, n_after/n_before*100))\n",
    "\n",
    "ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2 = pd.read_feather(\"measures2.feather\")\n",
    "ms2[\"log_dir\"] = ms2[\"model_path\"].str.split(\"/\").str.get(1)\n",
    "del ms2[\"model_path\"]\n",
    "for col in [\"l2\", \"gradient_norm\", \"seg_total_variation\", \"seg_total_variation_derivative\",\n",
    "           \"test_accuracy\", \"test_loss\"]:\n",
    "    del ms[col]\n",
    "\n",
    "ms = pd.merge(ms, ms2, on=\"log_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"actual_epochs\", \"train_loss\", \"test_loss\"]\n",
    "cols = ms.columns\n",
    "\n",
    "trim = 0.1\n",
    "\n",
    "for col in cols:\n",
    "    if ms[col].dtype == \"object\":\n",
    "        continue\n",
    "    \n",
    "    data = ms.loc[(ms[col] >= ms[col].quantile(trim/2)) & (ms[col] <= ms[col].quantile(1-trim/2)), col]\n",
    "    \n",
    "    plt.hist(data, bins=20)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1 = ms.sort_values(\"samples_train\")\n",
    "groups = ms1.groupby([\"hidden_size\", \"batch_size\", \"iteration\"])\n",
    "\n",
    "measure_cols = [\"gradient_norm\",\n",
    "                \"path_length_f\", \"path_length_f_softmax\",\n",
    "                \"path_length_d\", \"path_length_d_softmax\"]\n",
    "ratios = groups.agg(lambda g: np.log10(g.iloc[0] / g.iloc[-1]))[measure_cols]\n",
    "\n",
    "ratios.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ms1 = ms[(ms[\"hidden_size\"] == 100) & (ms[\"batch_size\"] == 128)]\n",
    "# ms1 = ms.loc[(ms[\"batch_size\"] == 256)]\n",
    "ms1 = ms\n",
    "\n",
    "for measure in measure_cols + [\"train_accuracy\", \"test_accuracy\", \"l2\"]:\n",
    "#     ax = plt.subplot()\n",
    "#     ms1.loc[:,\"hidden_size_s\"] = ms1[\"hidden_size\"].astype(str) + \" units\"\n",
    "    sns.relplot(\n",
    "        data=ms1,\n",
    "        x=\"samples_train\",\n",
    "        y=measure,\n",
    "        hue=\"hidden_size\",\n",
    "#         col=\"batch_size\",\n",
    "        kind=\"line\",\n",
    "#                 ax=ax\n",
    "    )\n",
    "#     plt.show()\n",
    "# ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(os.path.join(ms.iloc[0][\"log_dir\"], \"model.h5\"))\n",
    "mnist = smooth.datasets.get_mnist()\n",
    "x = mnist.x_train[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "measures = smooth.measures.get_measures(\n",
    "    model,\n",
    "    mnist.x_test, mnist.y_test,\n",
    "    include_training_measures=False,\n",
    "    is_classification=True,\n",
    "    samples=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[(ms[\"hidden_size\"] == 10) & (ms[\"batch_size\"] == 128) & (ms[\"iteration\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mnist = smooth.datasets.get_mnist()\n",
    "mss = ms[(ms[\"hidden_size\"] == 10) & (ms[\"batch_size\"] == 128)\n",
    "#          & (ms[\"iteration\"] == 1)\n",
    "        ]\n",
    "ms2_dict = {}\n",
    "for i, row in tqdm.notebook.tqdm(list(mss.iterrows())):\n",
    "    model = tf.keras.models.load_model(os.path.join(row[\"log_dir\"], \"model.h5\"))\n",
    "    measures = smooth.measures.get_measures(\n",
    "        model,\n",
    "        mnist.x_test, mnist.y_test,\n",
    "        include_training_measures=False,\n",
    "        is_classification=True,\n",
    "        samples=100,\n",
    "    )\n",
    "    ms2_dict[i] = measures\n",
    "# measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ms2_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2[\"samples_train\"] = ms[\"samples_train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms2 = pd.DataFrame(ms2_dict).transpose()\n",
    "ms2[\"samples_train\"] = ms[\"samples_train\"]\n",
    "ms2[\"hidden_size\"] = ms[\"hidden_size\"]\n",
    "\n",
    "for measure in set(ms2.columns) - set([\"samples_train\", \"hidden_size\"]):\n",
    "#     ax = plt.subplot()\n",
    "#     ms1.loc[:,\"hidden_size_s\"] = ms1[\"hidden_size\"].astype(str) + \" units\"\n",
    "    sns.relplot(\n",
    "        data=ms2,\n",
    "        x=\"samples_train\",\n",
    "        y=measure,\n",
    "        hue=\"hidden_size\",\n",
    "#         col=\"batch_size\",\n",
    "        kind=\"line\",\n",
    "#                 ax=ax\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
