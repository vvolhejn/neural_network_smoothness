{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-dimensional Gaussian process datasets 2\n",
    "\n",
    "Fewer datasets, but more different numbers of training samples.\n",
    "\n",
    "Also contains first experiments using kernel ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices([gpus[1]], 'GPU')\n",
    "\n",
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0227_gp_nd/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_raw = pd.read_feather(\"measures.feather\")\n",
    "\n",
    "if \"error\" in ms_raw.columns:\n",
    "    print(\"Errors:\", len(ms_raw[~ms_raw[\"error\"].isnull()]))\n",
    "    ms_raw = ms_raw.loc[ms_raw[\"error\"].isnull()]\n",
    "\n",
    "ms_raw = smooth.analysis.expand_dataset_columns(ms_raw)\n",
    "ms = ms_raw\n",
    "\n",
    "# divergent_model_mask = (ms[\"loss\"] == np.inf) | (~(ms[\"train_loss\"] < 0.1))\n",
    "# print(\"Divergent models:\", len(ms[divergent_model_mask]))\n",
    "# ms = ms.loc[~divergent_model_mask]\n",
    "\n",
    "print(\"Remaining:\", len(ms))\n",
    "smooth.analysis.remove_constant_columns(ms, verbose=True)\n",
    "# ms = smooth.analysis.expand_dataset_columns(ms)\n",
    "ms.loc[:,\"log_dir\"] = ms[\"log_dir\"].str.split(\"/\").str.get(-1)\n",
    "\n",
    "ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"actual_epochs\", \"train_loss\", \"test_loss\"]\n",
    "cols = ms.columns\n",
    "\n",
    "trim = 0.0\n",
    "\n",
    "for col in cols:\n",
    "    if ms[col].dtype == \"object\":\n",
    "        continue\n",
    "    \n",
    "    data = ms.loc[(ms[col] >= ms[col].quantile(trim/2)) & (ms[col] <= ms[col].quantile(1-trim/2)), col]\n",
    "    \n",
    "    plt.hist(data, bins=20)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_path_length_f(dataset_name):\n",
    "    dataset = smooth.datasets.from_name(dataset_name)\n",
    "    n = len(dataset.x_test)\n",
    "    y = sorted(dataset.y_test.reshape((-1,)))\n",
    "#     cs = np.cumsum(y)[::-1]\n",
    "#     res = 0\n",
    "#     for i in range(n - 1):\n",
    "#         res += cs[i] - y[i] * (n - i)\n",
    "\n",
    "#     return res / (n ** 2)\n",
    "    res = 0\n",
    "    for a in y:\n",
    "        for b in y:\n",
    "            res += np.abs(a - b)\n",
    "    return res / (n ** 2)\n",
    "\n",
    "optimal_lengths = {}\n",
    "for seed in tqdm.notebook.tqdm(range(1, 6)):\n",
    "    optimal_lengths[seed] = get_optimal_path_length_f(\"gp-100-{}-1.0-77\".format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(optimal_lengths.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_palette(values):\n",
    "    values = sorted(values)\n",
    "    pal = dict(zip(values, sns.cubehelix_palette(len(values))))\n",
    "    return pal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1 = ms\n",
    "\n",
    "measure_cols = [\"gradient_norm\", \"path_length_d\", \"path_length_f\"]\n",
    "for measure in measure_cols + [\"l2\", \"train_loss\", \"test_loss\"]:\n",
    "#     ax = plt.subplot()\n",
    "#     ms1.loc[:,\"hidden_size_s\"] = ms1[\"hidden_size\"].astype(str) + \" units\"\n",
    "    sns.lineplot(\n",
    "        data=ms1,\n",
    "        x=\"samples_train\",\n",
    "        y=measure,\n",
    "        hue=\"hidden_size\",\n",
    "#         col=\"batch_size\",\n",
    "#         kind=\"line\",\n",
    "        palette=make_palette(ms1[\"hidden_size\"].unique()),\n",
    "#         sns.cubehelix_palette(8),\n",
    "#                 ax=ax\n",
    "    )\n",
    "    if measure == \"path_length_f\":\n",
    "        ol = np.mean(list(optimal_lengths.values()))\n",
    "        plt.plot([ms1[\"samples_train\"].min(), ms1[\"samples_train\"].max()], [ol, ol])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[\"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for seed in tqdm.notebook.tqdm(range(1, 6)):\n",
    "    datasets[seed] = smooth.datasets.from_name(\"gp-100-{}-1.0-1000\".format(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = df.append({\"a\":1, \"b\":\" x\"}, ignore_index=True)\n",
    "df = df.append({\"a\":1, \"b\":\" x\"}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(tf.losses.mean_squared_error(model.predict(dataset.x_test), dataset.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRRModel:\n",
    "    \n",
    "    def __init__(self, krr):\n",
    "        self.krr = krr\n",
    "    \n",
    "    def predict(self, x, batch_size=None):\n",
    "        # batch_size is a fake argument which is ignored\n",
    "        return self.krr.predict(x)\n",
    "\n",
    "\n",
    "def measure_krr(krr, dataset):\n",
    "    def mse(y1, y2):\n",
    "        return np.mean(tf.losses.mean_squared_error(y1, y2))\n",
    "\n",
    "    train_loss = mse(krr.predict(dataset.x_train), dataset.y_train)\n",
    "    test_loss = mse(krr.predict(dataset.x_test), dataset.y_test)\n",
    "    path_length_f = smooth.measures.path_length(KRRModel(krr), dataset.x_test)\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"path_length_f\": path_length_f,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sklearn.kernel_ridge\n",
    "\n",
    "ms_krr_l = []\n",
    "\n",
    "for seed in tqdm.notebook.tqdm(range(1, 6)[:1], desc=\"seed\"):\n",
    "    for iteration in tqdm.notebook.tqdm(range(3), desc=\"iteration\"):\n",
    "        for alpha in tqdm.notebook.tqdm([1, 0.01, 0.0001], leave=False, desc=\"alpha\"):\n",
    "            for degree in tqdm.notebook.tqdm([1, 2, 3, 4, 5], leave=False, desc=\"degree\"):\n",
    "                for samples in tqdm.notebook.tqdm(np.sort(ms[\"samples_train\"].unique()), leave=False, desc=\"samples\"):\n",
    "                    krr = sklearn.kernel_ridge.KernelRidge(\n",
    "                        alpha=alpha,\n",
    "                        kernel=\"poly\",\n",
    "                        degree=degree,\n",
    "                #     degree=len(dataset.x_train) + 10,\n",
    "                        coef0=1,\n",
    "                    )\n",
    "                    dataset = datasets[seed].subset(samples, keep_test_set=True)\n",
    "                    krr.fit(dataset.x_train, dataset.y_train)\n",
    "\n",
    "                    m = measure_krr(krr, dataset)\n",
    "                    m.update(\n",
    "                        seed=seed,\n",
    "                        alpha=alpha,\n",
    "                        degree=degree,\n",
    "                        samples_train=samples,\n",
    "                        iteration=iteration,\n",
    "                    )\n",
    "                    ms_krr_l.append(m)\n",
    "                #     y_pred = krr.predict(dataset.x_test)\n",
    "                #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "ms_krr = pd.DataFrame(ms_krr_l)\n",
    "# ms_krr = ms_krr.loc[ms_krr[\"degree\"] > 4]\n",
    "# ms_krr[\"samples_train\"] = np.sort(ms[\"samples_train\"].unique())[ms_krr.index % 10]\n",
    "\n",
    "for measure in [\"train_loss\", \"test_loss\", \"path_length_f\"]:\n",
    "    ax = plt.subplot()\n",
    "    sns.lineplot(\n",
    "        data=ms_krr,\n",
    "        x=\"samples_train\",\n",
    "        y=measure,\n",
    "        hue=\"degree\",\n",
    "        style=\"alpha\",\n",
    "    #         col=\"batch_size\",\n",
    "    #         kind=\"line\",\n",
    "        palette=make_palette(ms_krr[\"degree\"].unique()),\n",
    "    #         sns.cubehelix_palette(8),\n",
    "    #                 ax=ax\n",
    "    )\n",
    "    if measure in [\"train_loss\", \"test_loss\"]:\n",
    "        ax.set_yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.loc[(ms[\"hidden_size\"] == 30) & (ms[\"seed\"] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_krr.loc[(ms_krr[\"degree\"] == 5) & (ms_krr[\"alpha\"] == 0.01)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare(ms_dict):\n",
    "    l = []\n",
    "    for name, ms_cur in sorted(ms_dict.items()):\n",
    "        ms_cur.loc[:,\"source\"] = name\n",
    "        l.append(ms_cur)\n",
    "#     ms1.loc[:,\"source\"] = \"a\"\n",
    "#     ms2.loc[:,\"source\"] = \"b\"\n",
    "    ms_both = pd.concat(l)\n",
    "    ms_both = ms_both.loc[ms_both[\"samples_train\"] <= 200]\n",
    "    \n",
    "    for measure in [\"train_loss\", \"test_loss\", \"path_length_f\"]:\n",
    "        ax = plt.subplot()\n",
    "        sns.lineplot(\n",
    "            data=ms_both,\n",
    "            x=\"samples_train\",\n",
    "            y=measure,\n",
    "            hue=\"source\",\n",
    "#             style=\"alpha\",\n",
    "        #         col=\"batch_size\",\n",
    "        #         kind=\"line\",\n",
    "#             palette=make_palette(ms_krr[\"degree\"].unique()),\n",
    "        #         sns.cubehelix_palette(8),\n",
    "        #                 ax=ax\n",
    "        )\n",
    "        if measure in [\"train_loss\", \"test_loss\"]:\n",
    "            ax.set_yscale(\"log\")\n",
    "        plt.show()\n",
    "    return \n",
    "\n",
    "plot_compare({\n",
    "    \"nn, hs=010\": ms.loc[(ms[\"hidden_size\"] == 10) & (ms[\"seed\"] == 1)],\n",
    "    \"nn, hs=030\": ms.loc[(ms[\"hidden_size\"] == 30) & (ms[\"seed\"] == 1)],\n",
    "    \"nn, hs=100\": ms.loc[(ms[\"hidden_size\"] == 100) & (ms[\"seed\"] == 1)],\n",
    "    \"krr, deg=1\": ms_krr.loc[(ms_krr[\"degree\"] == 1) & (ms_krr[\"alpha\"] == 0.0001)],\n",
    "    \"krr, deg=2\": ms_krr.loc[(ms_krr[\"degree\"] == 2) & (ms_krr[\"alpha\"] == 0.0001)],\n",
    "    \"krr, deg=3\": ms_krr.loc[(ms_krr[\"degree\"] == 3) & (ms_krr[\"alpha\"] == 0.0001)],\n",
    "    \"krr, deg=4\": ms_krr.loc[(ms_krr[\"degree\"] == 4) & (ms_krr[\"alpha\"] == 0.0001)],\n",
    "    \"krr, deg=5\": ms_krr.loc[(ms_krr[\"degree\"] == 5) & (ms_krr[\"alpha\"] == 0.0001)],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(np.log10(5), np.log10(1000), 20).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"../0228-162015/measures.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
