{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-dimensional Gaussian process datasets\n",
    "\n",
    "A first exploration of high-dimensional Gaussian processes. We train a lot of models on a lot of GP datasets of various dimensions, lengthscales and training samples.\n",
    "\n",
    "We also have some preliminary results of whether smooth functions are learned. We compute the ratio of the roughness measures for 10 training samples vs 1000 training samples. If smooth functions are learned, we would expect the roughness measure to increase or stay roughly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices([gpus[1]], 'GPU')\n",
    "\n",
    "sys.path.append(\"/nfs/scistore12/chlgrp/vvolhejn/smooth\")\n",
    "\n",
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0226_gp_nd/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_raw = pd.read_feather(\"measures.feather\")\n",
    "\n",
    "print(\"Errors:\", len(ms_raw[~ms_raw[\"error\"].isnull()]))\n",
    "ms_raw = ms_raw.loc[ms_raw[\"error\"].isnull()]\n",
    "\n",
    "ms_raw = smooth.analysis.expand_dataset_columns(ms_raw)\n",
    "ms = ms_raw\n",
    "\n",
    "divergent_model_mask = (ms[\"loss\"] == np.inf) | (~(ms[\"train_loss\"] < 0.1))\n",
    "print(\"Divergent models:\", len(ms[divergent_model_mask]))\n",
    "ms = ms.loc[~divergent_model_mask]\n",
    "\n",
    "print(\"Remaining:\", len(ms))\n",
    "smooth.analysis.remove_constant_columns(ms, verbose=True)\n",
    "# ms = smooth.analysis.expand_dataset_columns(ms)\n",
    "ms.loc[:,\"log_dir\"] = ms[\"log_dir\"].str.split(\"/\").str.get(-1)\n",
    "\n",
    "for d in sorted(ms_raw[\"dim\"].unique()):\n",
    "    n_before = len(ms_raw[ms_raw[\"dim\"] == d])\n",
    "    n_after = len(ms[ms[\"dim\"] == d])\n",
    "    print(\"For dim {}:\\t{}/{}\\t({:.0f}%) remain\".format(d, n_after, n_before, n_after/n_before*100))\n",
    "\n",
    "ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[\"hidden_size\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ms.columns:\n",
    "    if ms[col].nunique() <= 20:\n",
    "        sns.countplot(ms[col])\n",
    "        plt.show()\n",
    "\n",
    "# plt.hist(ms[\"dim\"].astype(str))\n",
    "# plt.bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.loc[ms[\"gradient_norm\"] < ,\"gradient_norm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"actual_epochs\", \"train_loss\", \"test_loss\"]\n",
    "cols = ms.columns\n",
    "\n",
    "trim = 0.1\n",
    "\n",
    "for col in cols:\n",
    "    if ms[col].dtype == \"object\":\n",
    "        continue\n",
    "    \n",
    "    data = ms.loc[(ms[col] > ms[col].quantile(trim/2)) & (ms[col] < ms[col].quantile(1-trim/2)), col]\n",
    "    \n",
    "    plt.hist(data, bins=20)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1 = ms.sort_values(\"samples_train\")\n",
    "groups = ms1.groupby([\"dim\", \"lengthscale\", \"hidden_size\", \"init_scale\", \"learning_rate\"])\n",
    "\n",
    "measure_cols = [\"gradient_norm\", \"seg_total_variation\", \"seg_total_variation_derivative\"]\n",
    "ratios = groups.agg(lambda g: np.log10(g.iloc[0] / g.iloc[-1]))[measure_cols]\n",
    "\n",
    "ratios.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[(ms[\"hidden_size\"] == 1000) & (ms[\"init_scale\"] == 1) & (ms[\"learning_rate\"] == 0.01)\n",
    "  & (ms[\"lengthscale\"] ==1.) & (ms[\"dim\"] == 128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure in measure_cols:\n",
    "    n_bins = 10\n",
    "    bins = np.logspace(-2, 2, 20)\n",
    "    plt.hist(ratios[measure], bins=np.linspace(-2, 2, 30))\n",
    "#     plt.xscale(\"log\")\n",
    "    plt.title(\"log ratio of {}\".format(measure))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Percentage of cases where ratio < 1: {:.1f}%\".format(\n",
    "        (ratios[measure] < np.log10(1)).sum() / len(ratios) * 100\n",
    "    ))\n",
    "    print(\"Percentage of cases where ratio < 1.5: {:.1f}%\".format(\n",
    "        (ratios[measure] < np.log10(1.5)).sum() / len(ratios) * 100\n",
    "    ))\n",
    "    print(\"90th percentile: ratio is {:.2f}\".format(\n",
    "        10 ** ratios[measure].quantile(0.9)\n",
    "#         (ratios[measure] < np.log(1.5)).sum() / len(ratios) * 100\n",
    "    ))\n",
    "#     print(np.sum(ratios[measure]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(np.log10(10), np.log10(1000), 10).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
