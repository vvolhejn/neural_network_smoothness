{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking init scale\n",
    "\n",
    "Our initialization method is somewhat non-standard (due to my mistake):\n",
    "\n",
    "```python\n",
    "model.add(\n",
    "        layers.Dense(\n",
    "            hidden_size,\n",
    "            activation=activation,\n",
    "            kernel_initializer=VarianceScaling(scale=init_scale, mode=\"fan_out\"), # this\n",
    "            bias_initializer=VarianceScaling(scale=init_scale, mode=\"fan_out\"), # this\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        layers.Dense(\n",
    "            dataset.n_classes if classification else 1,\n",
    "            kernel_initializer=VarianceScaling(scale=init_scale, mode=\"fan_in\"), # this\n",
    "            bias_initializer=VarianceScaling(scale=init_scale, mode=\"fan_in\"), # this\n",
    "            activation=None,\n",
    "        )\n",
    "    )\n",
    "```\n",
    "\n",
    "This is to check whether it has a significant effect on performance, namely, what happens if we comment out the lines annotated with `# this`. The results of such training are in `0402_check_init_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.config\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/\")\n",
    "\n",
    "ms1 = pd.read_feather(\"0326_mnist_binary/measures.feather\")\n",
    "ms1[\"init_type\"] = \"ours\"\n",
    "ms2 = pd.read_feather(\"0402_check_init_scale/measures.feather\")\n",
    "ms2[\"init_type\"] = \"standard\"\n",
    "\n",
    "# ms = pd.concat([ms1, ms2], sort=False)\n",
    "# ms = ms.reset_index(drop=True)\n",
    "\n",
    "# print(\"Removing {} entries\".format(sum(ms[\"gradient_norm_test\"].isna())))\n",
    "ms1 = ms1[~ms1[\"gradient_norm_test\"].isna()]\n",
    "ms2 = ms2[~ms2[\"gradient_norm_test\"].isna()]\n",
    "# ms[\"model.weights_product_reg_coef\"] = ms[\"model.weights_product_reg_coef\"].fillna(value=0)\n",
    "\n",
    "smooth.analysis.remove_constant_columns(ms1, verbose=True)\n",
    "smooth.analysis.remove_constant_columns(ms2, verbose=True)\n",
    "\n",
    "ms1 = ms1[\n",
    "    (ms1[\"model.hidden_size\"] == 256) &\n",
    "    (ms1[\"model.gradient_norm_reg_coef\"] == 0) &\n",
    "    (ms1[\"model.weights_product_reg_coef\"] == 0)\n",
    "]\n",
    "\n",
    "ms1 = ms1.set_index(\"dataset.name\")\n",
    "ms2 = ms2.set_index(\"dataset.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1.join(ms2, lsuffix=\"_l\", rsuffix=\"_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ms1.join(ms2, lsuffix=\"_l\", rsuffix=\"_r\")\n",
    "\n",
    "for measure in [\"loss_train\", \"loss_test\", \"gradient_norm_test\", \"weights_product\"]:\n",
    "    grid = sns.relplot(\n",
    "        data=ms,\n",
    "        x=\"{}_l\".format(measure),\n",
    "        y=\"{}_r\".format(measure),\n",
    "    )\n",
    "    ax = grid.axes[0][0]\n",
    "    if \"loss\" in measure:\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        lim = (\n",
    "            min(ms[\"{}_l\".format(measure)].min(), ms[\"{}_r\".format(measure)].min()),\n",
    "            max(ms[\"{}_l\".format(measure)].max(), ms[\"{}_r\".format(measure)].max()),\n",
    "        )\n",
    "        ax.set_xlim(lim)\n",
    "        ax.set_ylim(lim)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms[\"loss_train_l\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
