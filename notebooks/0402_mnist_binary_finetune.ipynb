{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoother models through changing hyperparameters\n",
    "\n",
    "What happens to smoothness if we train models with different hyperparams? Specifically, we might try to decrease the init scale as this has yielded smoother models in the past. This is an updated version of this experiment to see how this interacts with explicit regularization. Can we get smoother models just by varying the hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport smooth.config\n",
    "%aimport smooth.datasets\n",
    "%aimport smooth.model\n",
    "%aimport smooth.analysis\n",
    "%aimport smooth.callbacks\n",
    "%aimport smooth.measures\n",
    "%aimport smooth.util\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/\")\n",
    "\n",
    "# ms = pd.read_feather(\"0326_mnist_binary/measures.feather\")\n",
    "# ms2 = pd.read_feather(\"0329_mnist_binary_gradient_norm/measures.feather\")\n",
    "\n",
    "ms_finetune = pd.read_feather(\"./0402_mnist_binary_finetune/measures.feather\")\n",
    "ms_finetune[\"kind\"] = \"IS: \" + ms_finetune[\"model.init_scale\"].map(str) + \", LR: \" + ms_finetune[\"model.learning_rate\"].map(str)\n",
    "\n",
    "ms_finetune_2 = pd.read_feather(\"./0406_mnist_binary_finetune_wp/measures.feather\")\n",
    "\n",
    "ms = pd.concat([ms_finetune, ms_finetune_2], sort=False)\n",
    "# ms = pd.concat([ms2, ms_finetune], sort=False)\n",
    "ms = ms.reset_index(drop=True)\n",
    "\n",
    "# print(\"Removing {} entries\".format(sum(ms[\"gradient_norm_test\"].isna())))\n",
    "# ms = ms[~ms[\"gradient_norm_test\"].isna()]\n",
    "ms[\"model.weights_product_reg_coef\"] = ms[\"model.weights_product_reg_coef\"].fillna(value=0)\n",
    "\n",
    "smooth.analysis.remove_constant_columns(ms, verbose=True)\n",
    "\n",
    "ms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_measure(ms, measure_name, bins=10):\n",
    "    kinds = sorted(ms[\"kind\"].unique())\n",
    "    data = ms[[measure_name, \"kind\"]].copy()\n",
    "    \n",
    "    if \"loss\" in measure_name:\n",
    "        log_measure_name = \"log10_{}\".format(measure_name)\n",
    "        data[log_measure_name] = np.log10(ms[measure_name])\n",
    "        measure_name = log_measure_name\n",
    "    \n",
    "    for kind in kinds:\n",
    "        grid = sns.distplot(\n",
    "            data.loc[data[\"kind\"] == kind, measure_name],\n",
    "            label=kind,\n",
    "            hist_kws={\n",
    "                \"range\": (data[measure_name].min(), data[measure_name].max()),\n",
    "            },\n",
    "            bins=bins,\n",
    "        )\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure in [\n",
    "            \"loss_train\", \"loss_test\",\n",
    "            \"gradient_norm_test\",\n",
    "            \"weights_product\",\n",
    "            \"path_length_f_test\",\n",
    "            \"path_length_d_test\",\n",
    "        ]:\n",
    "    plot_measure(ms_finetune, measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = ms.loc[ms[\"model.init_scale\"] == 0.1]\n",
    "ms[\"kind\"] = (\"gn: \" + ms[\"model.gradient_norm_reg_coef\"].map(str) +\n",
    "              \", wp: \" + ms[\"model.weights_product_reg_coef\"].map(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure in [\n",
    "            \"loss_train\", \"loss_test\",\n",
    "            \"gradient_norm_test\",\n",
    "            \"weights_product\",\n",
    "            \"path_length_f_test\",\n",
    "            \"path_length_d_test\",\n",
    "        ]:\n",
    "    plot_measure(ms, measure, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
