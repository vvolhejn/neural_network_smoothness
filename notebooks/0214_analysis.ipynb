{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1080 models trained on MNIST\n",
    "\n",
    "For these models, we try to check whether our measures are a predictor of generalization (which corresponds simply to the test error here, since the networks all achieve very low training error).\n",
    "\n",
    "However, the test accuracy is similar for all models as well so it is questionable whether these results are meaningful.\n",
    "\n",
    "## Also,\n",
    "\n",
    "first experiments with segment-based measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# If we don't need CUDA, do this before importing TF\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices([gpus[1]], 'GPU')\n",
    "\n",
    "sys.path.append(\"/nfs/scistore12/chlgrp/vvolhejn/smooth\")\n",
    "\n",
    "os.chdir(\"/nfs/scistore12/chlgrp/vvolhejn/smooth/logs/0214_mnist_1080/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"measures.feather\")\n",
    "df[\"model_dir\"] = df[\"log_dir\"].str.split(\"/\").str.get(2)\n",
    "del df[\"epochs\"]\n",
    "del df[\"log_dir\"]\n",
    "# These metrics are present in df2:\n",
    "del df[\"gradient_norm\"]\n",
    "del df[\"l2\"]\n",
    "\n",
    "df2 = pd.read_feather(\"measures3.feather\")\n",
    "df2[\"model_dir\"] = df2[\"model_path\"].str.split(\"/\").str.get(2)\n",
    "del df2[\"model_path\"]\n",
    "\n",
    "df = df.merge(df2, on=\"model_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype == \"object\":\n",
    "        continue\n",
    "    plt.hist(df[col], bins=20)\n",
    "    plt.title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_training_speed(hparam):\n",
    "    hparam_vals = sorted(df[hparam].unique())\n",
    "    for hparam_val in hparam_vals:\n",
    "        sns.distplot(\n",
    "            df.loc[df[hparam] == hparam_val, \"actual_epochs\"],\n",
    "            hist=False,\n",
    "            label=str(hparam_val)\n",
    "        )\n",
    "    plt.title(\"Epochs to convergence by {}\".format(hparam))\n",
    "    plt.show()\n",
    "\n",
    "compare_training_speed(\"batch_size\")\n",
    "compare_training_speed(\"init_scale\")\n",
    "compare_training_speed(\"learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smooth.analysis\n",
    "import smooth.measures\n",
    "import smooth.datasets\n",
    "mnist = smooth.datasets.get_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ken = smooth.analysis.get_kendall_coefs(\n",
    "    df,\n",
    "    [\"batch_size\", \"hidden_size\", \"init_scale\", \"learning_rate\", \"iteration\"],\n",
    "    \"val_accuracy\",\n",
    "    [\"accuracy\", \"actual_epochs\", \"gradient_norm\", \"l2\", \"loss\", \"val_loss\",\n",
    "     \"seg_total_variation\", \"seg_total_variation_derivative\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ken.astype(\"float32\").round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D-based measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_1d_measure(model, data, measure_f):\n",
    "    single_seg_results = []\n",
    "    for i in tqdm.notebook.tqdm(range(500)):\n",
    "        single_seg_results.append(measure_f(model, data, n_segments=1, n_samples_per_segment=100))\n",
    "    plt.hist(single_seg_results, bins=70)\n",
    "    plt.title(\"Distribution of measure when sampling a single segment\")\n",
    "    plt.show()\n",
    "    \n",
    "    n_samples_x = list(range(10, 500, 10))\n",
    "    n_samples_y = []\n",
    "    for n_samples in tqdm.notebook.tqdm(n_samples_x):\n",
    "        n_samples_y.append(measure_f(model, data, n_segments=30, n_samples_per_segment=n_samples))\n",
    "    \n",
    "    plt.plot(n_samples_x, n_samples_y)\n",
    "    plt.title(\"Distribution of measure when varying n_samples_per_segment\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smooth.datasets import mnist\n",
    "\n",
    "_model = tf.keras.models.load_model(\"bs=512_e=20000_hs=501_is=3.0_i=1_lr=0.01/model.h5\")\n",
    "analyze_1d_measure(_model, mnist.x_test, smooth.measures.segments_total_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "m2 = partial(smooth.measures.segments_total_variation, derivative=True)\n",
    "analyze_1d_measure(_model, mnist.x_test, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measures(log_dir):\n",
    "    path = os.path.join(log_dir, \"model.h5\")\n",
    "    model = tf.keras.models.load_model(path)\n",
    "    return {\n",
    "        \"total_variation\":\n",
    "            smooth.measures.segments_total_variation(model, mnist.x_test, n_segments=100),\n",
    "        \"total_variation_derivative\":\n",
    "            smooth.measures.segments_total_variation(\n",
    "                model, mnist.x_test,\n",
    "                n_segments=100, n_samples_per_segment=100, derivative=True\n",
    "            ),\n",
    "    }\n",
    "\n",
    "# get_measures(os.path.basename(df.iloc[0][\"log_dir\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total_variation\"] = None\n",
    "df[\"total_variation_derivative\"] = None\n",
    "\n",
    "for i in tqdm.notebook.tqdm(df.index):\n",
    "    log_dir = os.path.basename(df[\"log_dir\"][i])\n",
    "    measures = get_measures(log_dir)\n",
    "    df.loc[\"total_variation\", i] = measures[\"total_variation\"]\n",
    "    df.loc[\"total_variation_derivative\", i] = measures[\"total_variation_derivative\"]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df2.corr().round(2)\n",
    "corr.style.background_gradient(axis=None)\n",
    "# corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = tf.keras.models.load_model(\"./bs=128_e=20000_hs=1002_is=0.3_i=0_lr=0.003/model.h5\")\n",
    "from smooth.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _total_variation(samples, batch=False):\n",
    "    \"\"\"\n",
    "    Given evenly spaced samples of a function's values, computes an approximation\n",
    "    of the total variation, that is the sum of the distances of consecutive samples.\n",
    "\n",
    "    For scalar samples, this means the sum of absolute values of the first difference,\n",
    "    for vector-valued functions we sum the l2 norms of the first difference.\n",
    "\n",
    "    >>> _total_variation([1, 2, 3, 1])\n",
    "    4.0\n",
    "    >>> print(\"{:.3f}\".format(_total_variation([[0, 0], [1, 1], [1, 2]])))\n",
    "    2.414\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        samples = np.array([samples])\n",
    "    res = np.diff(samples, axis=1)\n",
    "    if res.ndim == 2:\n",
    "        res = res[:, :, np.newaxis]\n",
    "    res = np.linalg.norm(res, axis=2)\n",
    "    res = np.sum(res, axis=1)\n",
    "    \n",
    "    if not batch:\n",
    "        assert len(res) == 1\n",
    "        return res[0]\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "\n",
    "def _interpolate(a, b, n_samples):\n",
    "    \"\"\"\n",
    "    >>> _interpolate(1, 2, 3).tolist()\n",
    "    [1.0, 1.5, 2.0]\n",
    "    >>> _interpolate([0, 3], [3, 0], 4).tolist()\n",
    "    [[0.0, 3.0], [1.0, 2.0], [2.0, 1.0], [3.0, 0.0]]\n",
    "    >>> _interpolate([[0, 2], [1, 1]], [[2, 0], [2, 2]], 3).tolist()\n",
    "    [[[0.0, 2.0], [1.0, 1.0]], [[1.0, 1.0], [1.5, 1.5]], [[2.0, 0.0], [2.0, 2.0]]]\n",
    "    \"\"\"\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    assert a.shape == b.shape\n",
    "    w = np.linspace(0, 1, n_samples)\n",
    "    res = np.outer(1 - w, a) + np.outer(w, b)\n",
    "    res = np.reshape(res, (-1,) + a.shape)\n",
    "    return res\n",
    "\n",
    "def _segment_total_variation(\n",
    "    model: tf.keras.Model, x1, x2, n_samples, derivative: bool\n",
    "):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    n_segments = len(x1)\n",
    "    assert x1.shape == x2.shape\n",
    "    samples = _interpolate(x1, x2, n_samples)\n",
    "    samples_flat = np.reshape(samples, (n_samples * n_segments,) + samples.shape[2:])\n",
    "\n",
    "    if not derivative:\n",
    "        output_flat = model.predict(samples_flat)\n",
    "        output = np.reshape(output_flat, (n_samples, n_segments) + output_flat.shape[1:])\n",
    "        # at this point, `output` has shape (n_segments, n_samples, n_classes)\n",
    "    else:\n",
    "        with tf.GradientTape() as g:\n",
    "            x = tf.constant(samples_flat)\n",
    "            g.watch(x)\n",
    "            y = model(x)\n",
    "        output_flat = g.batch_jacobian(y, x)\n",
    "        # We just stretch the Jacobian into a single vector and take its total variation\n",
    "        # (meaning we sum the Frobenius norms of the first difference)\n",
    "        # Does this make any sense mathematically?\n",
    "        output_flat = np.reshape(output_flat, (len(samples_flat), -1))\n",
    "\n",
    "    output = np.reshape(output_flat, (n_samples, n_segments) + output_flat.shape[1:])\n",
    "    output = np.swapaxes(output, 0, 1)\n",
    "    return _total_variation(output, batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _segment_total_variation(\n",
    "#     _model,\n",
    "#     [mnist.x_test[0], mnist.x_test[2], mnist.x_test[4]],\n",
    "#     [mnist.x_test[1], mnist.x_test[3], mnist.x_test[5]],\n",
    "#     100, False,\n",
    "# )\n",
    "\n",
    "_segment_total_variation(\n",
    "    _model,\n",
    "    [mnist.x_test[0],mnist.x_test[2]],\n",
    "    [mnist.x_test[1],mnist.x_test[3]],\n",
    "    100, False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _segment_total_variation0(\n",
    "    model: tf.keras.Model, x1, x2, n_samples, derivative: bool\n",
    "):\n",
    "    global _d1\n",
    "    samples = _interpolate(x1, x2, n_samples)\n",
    "    if not derivative:\n",
    "        output = model.predict(samples)\n",
    "    else:\n",
    "        with tf.GradientTape() as g:\n",
    "            x = tf.constant(samples)\n",
    "            g.watch(x)\n",
    "            y = model(x)\n",
    "        output = g.batch_jacobian(y, x)\n",
    "        # We just stretch the Jacobian into a single vector and take its total variation\n",
    "        # (meaning we sum the Frobenius norms of the first difference)\n",
    "        # Does this make any sense mathematically?\n",
    "        output = np.reshape(output, (n_samples, -1))\n",
    "    return _total_variation(output)\n",
    "\n",
    "_segment_total_variation0(\n",
    "    _model,\n",
    "    mnist.x_test[2],\n",
    "    mnist.x_test[3],\n",
    "    100, False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(range(10,20))\n",
    "a[[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
